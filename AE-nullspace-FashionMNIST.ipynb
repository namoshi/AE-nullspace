{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Encoderを用いた特徴ベクトルの分解\n",
    "\n",
    "特徴ベクトル$f$をAuto-Encoderを用いて，識別空間のベクトル$u$とカーネル空間のベクトル$v=f-u$に分解するプログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/938], Classification Loss: 0.5554, Reconstruction Loss: 1.6924, Combined Loss: 0.7247\n",
      "Epoch [1/10], Step [200/938], Classification Loss: 0.5458, Reconstruction Loss: 0.5696, Combined Loss: 0.6027\n",
      "Epoch [1/10], Step [300/938], Classification Loss: 0.3814, Reconstruction Loss: 0.4243, Combined Loss: 0.4238\n",
      "Epoch [1/10], Step [400/938], Classification Loss: 0.2794, Reconstruction Loss: 0.2853, Combined Loss: 0.3080\n",
      "Epoch [1/10], Step [500/938], Classification Loss: 0.5322, Reconstruction Loss: 0.2178, Combined Loss: 0.5540\n",
      "Epoch [1/10], Step [600/938], Classification Loss: 0.3422, Reconstruction Loss: 0.1607, Combined Loss: 0.3583\n",
      "Epoch [1/10], Step [700/938], Classification Loss: 0.2942, Reconstruction Loss: 0.1439, Combined Loss: 0.3086\n",
      "Epoch [1/10], Step [800/938], Classification Loss: 0.2829, Reconstruction Loss: 0.1421, Combined Loss: 0.2971\n",
      "Epoch [1/10], Step [900/938], Classification Loss: 0.2742, Reconstruction Loss: 0.0989, Combined Loss: 0.2841\n",
      "Epoch [2/10], Step [100/938], Classification Loss: 0.3731, Reconstruction Loss: 0.1051, Combined Loss: 0.3836\n",
      "Epoch [2/10], Step [200/938], Classification Loss: 0.2237, Reconstruction Loss: 0.1023, Combined Loss: 0.2339\n",
      "Epoch [2/10], Step [300/938], Classification Loss: 0.2790, Reconstruction Loss: 0.1112, Combined Loss: 0.2901\n",
      "Epoch [2/10], Step [400/938], Classification Loss: 0.3809, Reconstruction Loss: 0.1090, Combined Loss: 0.3918\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# デバイスの設定 (GPUが利用可能であればGPUを使用)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Fashion MNIST データの読み込みとデータ拡張\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# CNNモデルの定義 (Encoder + Classifier + Decoder)\n",
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self, feature_dim=128, bottleneck_dim=32):\n",
    "        super(CNNAutoencoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, feature_dim)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(feature_dim, 10)  # Classifier\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Linear(10, feature_dim)  # Decoder: 10次元 -> feature_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        feature_vector = self.relu3(self.fc1(x))\n",
    "\n",
    "        # Classifier\n",
    "        class_output = self.fc2(feature_vector)\n",
    "\n",
    "        # Decoder\n",
    "        reconstructed_feature_vector = self.decoder(class_output)\n",
    "\n",
    "        return class_output, feature_vector, reconstructed_feature_vector\n",
    "\n",
    "# モデル、損失関数、最適化手法の定義\n",
    "feature_dim = 64\n",
    "cnn_autoencoder_model = CNNAutoencoder(feature_dim).to(device)\n",
    "\n",
    "criterion_cnn = nn.CrossEntropyLoss()\n",
    "criterion_autoencoder = nn.MSELoss()\n",
    "optimizer = optim.Adam(cnn_autoencoder_model.parameters(), lr=0.001)\n",
    "\n",
    "lambda_reg = 0.1  # 正則化の強さ\n",
    "\n",
    "# 学習\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "reconstruction_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_autoencoder_model.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_reconstruction_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 順伝播\n",
    "        outputs, feature_vectors, reconstructed_feature_vector = cnn_autoencoder_model(images)\n",
    "\n",
    "        # 損失の計算\n",
    "        loss_classification = criterion_cnn(outputs, labels)\n",
    "        loss_reconstruction = criterion_autoencoder(reconstructed_feature_vector, feature_vectors) #特徴ベクトルを再構成するように修正\n",
    "\n",
    "        # 正則化項を損失に加える\n",
    "        loss = loss_classification + lambda_reg * loss_reconstruction\n",
    "\n",
    "        # 勾配計算とパラメータ更新\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss_classification.item()\n",
    "        epoch_reconstruction_loss += loss_reconstruction.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Classification Loss: {loss_classification.item():.4f}, Reconstruction Loss: {loss_reconstruction.item():.4f}, Combined Loss: {loss.item():.4f}')\n",
    "\n",
    "    train_losses.append(epoch_train_loss / num_batches)\n",
    "    reconstruction_losses.append(epoch_reconstruction_loss / num_batches)\n",
    "\n",
    "print('Finished Training.')\n",
    "\n",
    "# 学習曲線のプロット\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Classification Loss')\n",
    "plt.plot(reconstruction_losses, label='Reconstruction Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curves')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 評価\n",
    "cnn_autoencoder_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs, _, _ = cnn_autoencoder_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the CNN on the test images: {100 * correct / total:.2f} %')\n",
    "\n",
    "# 分解関数の定義 (ここではAutoencoderによる分解は行わないので、恒等写像として定義)\n",
    "def decompose_feature_vector(feature_vector):\n",
    "    u = torch.zeros_like(feature_vector) # ゼロベクトル\n",
    "    v = feature_vector # 特徴ベクトルそのもの\n",
    "    return u, v\n",
    "\n",
    "# 訓練データに対する特徴ベクトルと分解ベクトルの取得\n",
    "cnn_autoencoder_model.eval()\n",
    "feature_vectors = []\n",
    "u_vectors = []\n",
    "v_vectors = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs, f, _ = cnn_autoencoder_model(images)\n",
    "        u, v = decompose_feature_vector(f)\n",
    "\n",
    "        feature_vectors.append(f.cpu().numpy())\n",
    "        u_vectors.append(u.cpu().numpy())\n",
    "        v_vectors.append(v.cpu().numpy())\n",
    "        labels_list.append(labels.cpu().numpy())\n",
    "\n",
    "feature_vectors = np.concatenate(feature_vectors)\n",
    "u_vectors = np.concatenate(u_vectors)\n",
    "v_vectors = np.concatenate(v_vectors)\n",
    "labels_list = np.concatenate(labels_list)\n",
    "\n",
    "# PCAによる次元削減とプロット\n",
    "def plot_pca(data, labels, title):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(data)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(10):  # FashionMNISTのクラス数\n",
    "        plt.scatter(pca_result[labels == i, 0], pca_result[labels == i, 1], label=str(i), alpha=0.5)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# プロット\n",
    "plot_pca(feature_vectors, labels_list, 'PCA of Feature Vectors')\n",
    "plot_pca(u_vectors, labels_list, 'PCA of Component u')\n",
    "plot_pca(v_vectors, labels_list, 'PCA of Component v')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
